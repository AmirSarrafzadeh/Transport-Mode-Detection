{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21266,
     "status": "ok",
     "timestamp": 1625311102614,
     "user": {
      "displayName": "Amir sarrafzadeh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgF4jrw5H5CJhyyB1ApCoS9hQnBVQeXVXHtH8_pLQ=s64",
      "userId": "14181695720010602226"
     },
     "user_tz": -120
    },
    "id": "KPpdD3JqqFJ4",
    "outputId": "e8a9c09b-b5f1-4eb2-a214-38ed3d530d20"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "# Mount The Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 301,
     "status": "ok",
     "timestamp": 1625311105069,
     "user": {
      "displayName": "Amir sarrafzadeh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgF4jrw5H5CJhyyB1ApCoS9hQnBVQeXVXHtH8_pLQ=s64",
      "userId": "14181695720010602226"
     },
     "user_tz": -120
    },
    "id": "7Z4iBvkGqG_g",
    "outputId": "6773c249-ae65-4954-c5dc-65793a17cee6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries Are Imported\n"
     ]
    }
   ],
   "source": [
    "# Import Libraries\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time \n",
    "import datetime\n",
    "import pickle\n",
    "import random\n",
    "import math\n",
    "from collections import Counter\n",
    "import statistics \n",
    "from statistics import mode\n",
    "pd.options.mode.chained_assignment = None\n",
    "print(\"Libraries Are Imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1625311105451,
     "user": {
      "displayName": "Amir sarrafzadeh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgF4jrw5H5CJhyyB1ApCoS9hQnBVQeXVXHtH8_pLQ=s64",
      "userId": "14181695720010602226"
     },
     "user_tz": -120
    },
    "id": "wjk2T3wUqHE-"
   },
   "outputs": [],
   "source": [
    "# Define Path of Data\n",
    "path = '---------------------------------'\n",
    "os.chdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 8242,
     "status": "ok",
     "timestamp": 1625311113956,
     "user": {
      "displayName": "Amir sarrafzadeh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgF4jrw5H5CJhyyB1ApCoS9hQnBVQeXVXHtH8_pLQ=s64",
      "userId": "14181695720010602226"
     },
     "user_tz": -120
    },
    "id": "FSh1DtYiRC8U"
   },
   "outputs": [],
   "source": [
    "# Read The Labeled Data\n",
    "DF = '004_Preprocessed_Data.pickle'\n",
    "infile = open(DF,'rb')\n",
    "df = pickle.load(infile)\n",
    "infile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 2898,
     "status": "ok",
     "timestamp": 1625311116851,
     "user": {
      "displayName": "Amir sarrafzadeh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgF4jrw5H5CJhyyB1ApCoS9hQnBVQeXVXHtH8_pLQ=s64",
      "userId": "14181695720010602226"
     },
     "user_tz": -120
    },
    "id": "GczNua379v7q"
   },
   "outputs": [],
   "source": [
    "# Replace Names with Characters\n",
    "df.replace('bike', 'p', inplace=True)\n",
    "df.replace('bus', 'b', inplace=True)\n",
    "df.replace('car', 'c', inplace=True)\n",
    "df.replace('train', 't', inplace=True)\n",
    "df.replace('walk', 'w', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6134,
     "status": "ok",
     "timestamp": 1625311122968,
     "user": {
      "displayName": "Amir sarrafzadeh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgF4jrw5H5CJhyyB1ApCoS9hQnBVQeXVXHtH8_pLQ=s64",
      "userId": "14181695720010602226"
     },
     "user_tz": -120
    },
    "id": "qdiWwHKR9wAV",
    "outputId": "ae2c000c-f526-45cb-e375-c8ca1726d42a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mpu\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/3a/c4c04201c9cd8c5845f85915d644cb14b16200680e5fa424af01c411e140/mpu-0.23.1-py3-none-any.whl (69kB)\n",
      "\r",
      "\u001b[K     |████▊                           | 10kB 16.6MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▍                      | 20kB 16.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████                  | 30kB 14.6MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▉             | 40kB 13.2MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▌        | 51kB 7.6MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▏   | 61kB 7.3MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 71kB 4.5MB/s \n",
      "\u001b[?25hInstalling collected packages: mpu\n",
      "Successfully installed mpu-0.23.1\n"
     ]
    }
   ],
   "source": [
    "# Split All Data to Train and Test Set Randomly\n",
    "User_List = df['User'].unique().tolist()\n",
    "!pip install mpu\n",
    "\n",
    "import mpu\n",
    "random.seed(29)\n",
    "list_one = User_List\n",
    "\n",
    "list_one, list_two = mpu.consistent_shuffle(list_one,list_one)\n",
    "train_user = list_one[:45]\n",
    "test_user = list_one[45:]\n",
    "\n",
    "Train_Data = df[df['User'].isin(train_user)]\n",
    "Test_Data = df[df['User'].isin(test_user)]\n",
    "\n",
    "Train_data = []\n",
    "for i, g in Train_Data.groupby(['Trip','User']):\n",
    "  Train_data.append(g)\n",
    "\n",
    "Test_data = []\n",
    "for l, m in Test_Data.groupby(['Trip','User']):\n",
    "  Test_data.append(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oMN4HtHR-MI2"
   },
   "outputs": [],
   "source": [
    "# Define Extra Columns and Drop them\n",
    "extra_columns = ['Latitude', 'Longitude', 'Altitude', 'Date', 'User', 'TS', 'Trip', 'Mode', 'Distance', 'DT', 'Bearing', 'Cum_Distance']\n",
    "\n",
    "Train_Y_t = []\n",
    "for df in Train_data:      \n",
    "    Train_Y_t.append(df['Mode'].values)\n",
    "    df.drop(columns=extra_columns, inplace=True)\n",
    "\n",
    "Test_Y_t = []\n",
    "for df in Test_data:      \n",
    "    Test_Y_t.append(df['Mode'].values)\n",
    "    df.drop(columns=extra_columns, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ta-uapF_-MLu"
   },
   "outputs": [],
   "source": [
    "# Normalize Both Train and Test Datasets\n",
    "Train_Normalize = []\n",
    "for df in Train_data:      \n",
    "    Train_Normalize.append(df.values)\n",
    "\n",
    "Test_Normalize = []\n",
    "for df in Test_data:      \n",
    "    Test_Normalize.append(df.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_tMRMnwf-MPX"
   },
   "outputs": [],
   "source": [
    "# Define Maximum and Minimum Trip Sizes\n",
    "max_trip_size = 200\n",
    "min_trip_size = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_jmsjKPk-MSm"
   },
   "outputs": [],
   "source": [
    "# Break the Trips to Windows - Add Padding to Windows\n",
    "def break_trip(trip, trip_Y, max_trip_size):\n",
    "    length = max_trip_size\n",
    "    jump = length\n",
    "    split = [trip[i:i+length] for i in range(0,len(trip),jump)][:-1]+[trip[-length:]]\n",
    "    split_Y = [trip_Y[i:i+length] for i in range(0,len(trip_Y),jump)][:-1]+[trip_Y[-length:]] \n",
    "    return split, split_Y\n",
    "\n",
    "def padd_trip(trip, trip_Y, max_trip_size):\n",
    "    trip_padded = np.pad(trip, ((0, max_trip_size-trip.shape[0]), (0, 0)), 'constant')\n",
    "    trip_padded_Y = np.pad(trip_Y, (0, max_trip_size-trip.shape[0]), 'constant', constant_values=(0,0))\n",
    "    return trip_padded, trip_padded_Y\n",
    "\n",
    "Train_X = []\n",
    "Train_Y = []\n",
    "for i, trip in enumerate(Train_Normalize):\n",
    "    size_trip = trip.shape[0]\n",
    "    if  size_trip <= min_trip_size:   \n",
    "        continue\n",
    "    \n",
    "    if size_trip > max_trip_size:\n",
    "        trip_breaks, trip_breaks_Y = break_trip(trip, Train_Y_t[i], max_trip_size)\n",
    "        Train_X.extend(trip_breaks)\n",
    "        Train_Y.extend(trip_breaks_Y)            \n",
    "        \n",
    "    if size_trip <= max_trip_size and size_trip > min_trip_size:\n",
    "        trip_pad, trip_pad_Y = padd_trip(trip, Train_Y_t[i], max_trip_size)\n",
    "        Train_X.append(trip_pad)\n",
    "        Train_Y.append(Counter(Train_Y_t[i].flat).most_common(1)[0][0])   \n",
    "\n",
    "Test_X = []\n",
    "Test_Y = []\n",
    "for i, trip in enumerate(Test_Normalize):\n",
    "    size_trip = trip.shape[0]\n",
    "    if  size_trip <= min_trip_size:   \n",
    "        continue\n",
    "    \n",
    "    if size_trip > max_trip_size:\n",
    "        trip_breaks, trip_breaks_Y = break_trip(trip, Test_Y_t[i], max_trip_size)\n",
    "        Test_X.extend(trip_breaks)\n",
    "        Test_Y.extend(trip_breaks_Y)\n",
    "         \n",
    "    if size_trip <= max_trip_size and size_trip > min_trip_size:\n",
    "        trip_pad, trip_pad_Y = padd_trip(trip, Test_Y_t[i], max_trip_size)\n",
    "        Test_X.append(trip_pad)\n",
    "        Test_Y.append(Counter(Test_Y_t[i].flat).most_common(1)[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7QRHUbAm9wFU"
   },
   "outputs": [],
   "source": [
    "# Find the mode of each Single Window\n",
    "Train_M = []\n",
    "for i in Train_Y:\n",
    "  if len(i) == 1:\n",
    "    Train_M.append(i)\n",
    "  else:\n",
    "    lst = i.tolist()\n",
    "    Train_M.append(max(set(lst), key=lst.count))\n",
    "\n",
    "Test_M = []\n",
    "for i in Test_Y:\n",
    "  if len(i) == 1:\n",
    "    Test_M.append(i)\n",
    "  else:\n",
    "    lst = i.tolist()\n",
    "    Test_M.append(max(set(lst), key=lst.count))\n",
    "\n",
    "train_Y = pd.DataFrame(Train_M,columns=['LL'])\n",
    "test_Y = pd.DataFrame(Test_M,columns=['LL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V71ORtmSP0j0"
   },
   "outputs": [],
   "source": [
    "# Replace Characters with Names in Train Dataset\n",
    "train_Y.replace('p', 'bike', inplace=True)\n",
    "train_Y.replace('b', 'bus', inplace=True)\n",
    "train_Y.replace('c', 'car', inplace=True)\n",
    "train_Y.replace('t', 'train', inplace=True)\n",
    "train_Y.replace('w', 'walk', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CSpbtnoUQIdA"
   },
   "outputs": [],
   "source": [
    "# Replace Characters with Names in Test Dataset\n",
    "test_Y.replace('p', 'bike', inplace=True)\n",
    "test_Y.replace('b', 'bus', inplace=True)\n",
    "test_Y.replace('c', 'car', inplace=True)\n",
    "test_Y.replace('t', 'train', inplace=True)\n",
    "test_Y.replace('w', 'walk', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vl79bOKLQIfF"
   },
   "outputs": [],
   "source": [
    "# Save all Data in a Dictionary\n",
    "data_dict = dict()\n",
    "data_dict['Train_X'] = Train_X\n",
    "data_dict['Train_Y'] = train_Y\n",
    "data_dict['Test_X'] = Test_X\n",
    "data_dict['Test_Y'] = test_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4IXS_wBNP0ls"
   },
   "outputs": [],
   "source": [
    "# Define Train_X, Test_X. train_Y, and test_Y\n",
    "Train_X = np.asarray(data_dict['Train_X']).astype('float32')\n",
    "train_Y = np.asarray(data_dict['Train_Y'])\n",
    "Test_X = np.asarray(data_dict['Test_X']).astype('float32')\n",
    "test_Y = np.asarray(data_dict['Test_Y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HxOwCAyJQZL8"
   },
   "outputs": [],
   "source": [
    "# Apply One Hot Encoding To Train_Y and Test_Y\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "Train_Y = enc.fit_transform(train_Y.reshape(-1, 1)).toarray()\n",
    "print(Train_X.shape)\n",
    "print(Train_Y.shape)\n",
    "Test_Y = enc.fit_transform(test_Y.reshape(-1, 1)).toarray()\n",
    "print(Test_X.shape)\n",
    "print(Test_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pwwu1ZnOQZOS"
   },
   "outputs": [],
   "source": [
    "# Define Number of Features and Number of Classes\n",
    "num_features = Train_X.shape[-1]\n",
    "print(num_features)\n",
    "Modes = enc.categories_\n",
    "NoClass = len(Modes[0])\n",
    "print(NoClass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4ErNXup6QZTj"
   },
   "outputs": [],
   "source": [
    "# Import Deep Learning Libraries\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(42)\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Conv1D, Flatten, MaxPooling1D, BatchNormalization, Activation, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import metrics\n",
    "from scipy.stats import zscore\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(allow_soft_placement=True, log_device_placement=True))\n",
    "sess = print(tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True)))\n",
    "\n",
    "start_time = time.clock()\n",
    "np.random.seed(7)\n",
    "random.seed(7)\n",
    "\n",
    "trip_size = Train_X.shape[-2]\n",
    "trip_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "805Zcx5KxK-d"
   },
   "outputs": [],
   "source": [
    "# Define Kernel Size, Maxpooling Size, Stride\n",
    "kernel = 16\n",
    "pool = 4\n",
    "stride = 1\n",
    "Drop_Out = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ciZH8pgTKu4J"
   },
   "outputs": [],
   "source": [
    "# Structure of Model and Compile\n",
    "model = Sequential()\n",
    "model.add(Conv1D(64, kernel, strides=stride, padding='same', dilation_rate = 1, input_shape=(trip_size, num_features)))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Conv1D(64, kernel, strides=stride, padding='same', dilation_rate = 1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling1D(pool_size=pool))\n",
    "\n",
    "model.add(Dropout(Drop_Out))\n",
    "\n",
    "model.add(Conv1D(128, kernel, strides=stride, padding='same', dilation_rate = 1))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Conv1D(128, kernel, strides=stride, padding='same', dilation_rate = 1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling1D(pool_size=pool))\n",
    "\n",
    "model.add(Dropout(Drop_Out))\n",
    "\n",
    "model.add(Conv1D(256, kernel, strides=stride, padding='same', dilation_rate = 1))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Conv1D(256, kernel, strides=stride, padding='same', dilation_rate = 1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling1D(pool_size=pool))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dropout(Drop_Out))\n",
    "\n",
    "model.add(Dense(2048))\n",
    "model.add(Activation(\"relu\"))\n",
    "\n",
    "model.add(Dropout(Drop_Out))\n",
    "\n",
    "model.add(Dense(1024))\n",
    "model.add(Activation(\"relu\"))\n",
    "\n",
    "model.add(Dense(NoClass, activation='softmax'))\n",
    "\n",
    "EPOCHS = 100\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch % EPOCHS == 0 and epoch != 0:\n",
    "        print(\"[INFO] lr is  ... \", lr/10)                \n",
    "        return lr/10\n",
    "    else:\n",
    "        return lr\n",
    "\n",
    "# optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "optimizer = RMSprop(lr=0.001)\n",
    "# optimizer = SGD(lr=0.0001, momentum=0.9, decay=1e-4, nesterov=True)\n",
    "\n",
    "callback = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "hist = model.fit(Train_X, Train_Y, epochs=500, batch_size=64, validation_data=(Test_X, Test_Y), callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J7qblXbvQZa-"
   },
   "outputs": [],
   "source": [
    "# Model Summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o8EYRdL5Qqzn"
   },
   "outputs": [],
   "source": [
    "# Prediction of Model\n",
    "pred_Y = model.predict(Test_X)\n",
    "pred_Y_N = np.argmax(pred_Y, axis=1)\n",
    "Pred_Y = enc.fit_transform(pred_Y_N.reshape(-1, 1)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KDdUnweBQsGT"
   },
   "outputs": [],
   "source": [
    "# Apply One Hot Encoding Transform\n",
    "Pred_Y_N = enc.inverse_transform(Pred_Y)\n",
    "Test_Y_N = enc.inverse_transform(Test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yR7_zj_OQq2r"
   },
   "outputs": [],
   "source": [
    "print(confusion_matrix(Test_Y_N, Pred_Y_N))\n",
    "print(classification_report(Test_Y_N, Pred_Y_N))\n",
    "print(accuracy_score(Test_Y_N, Pred_Y_N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r3MZJIKGQq5p"
   },
   "outputs": [],
   "source": [
    "# Plt Confusion Matrix\n",
    "LABELS = ['bike','bus','car','train','walk']\n",
    "cm = confusion_matrix(Test_Y_N, Pred_Y_N)\n",
    "bg_color = (0.88,0.85,0.95)\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "plt.figure(figsize=(15,10))\n",
    "sn.heatmap(cm,xticklabels=LABELS, yticklabels=LABELS,annot=True, fmt=\"d\", cmap='jet', annot_kws={'size':15})\n",
    "plt.title('Confusion Matrix', fontsize = 20)\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.xlabel('Predicted', fontsize = 16)\n",
    "plt.ylabel('True', fontsize = 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tUimvpovQ7eW"
   },
   "outputs": [],
   "source": [
    "# Summarize History For Accuracy\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.plot(hist.history['accuracy'])\n",
    "plt.plot(hist.history['val_accuracy'])\n",
    "plt.title('Model Accuracy', fontsize = 20)\n",
    "plt.ylabel('Accuracy', fontsize = 16)\n",
    "plt.xlabel('Epoch', fontsize = 16)\n",
    "plt.legend(['Train', 'Test'], loc='upper left', fontsize = 16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LNATHP5AQ7hH"
   },
   "outputs": [],
   "source": [
    "# Summarize History For Loss\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.plot(hist.history['loss'])\n",
    "plt.plot(hist.history['val_loss'])\n",
    "plt.title('Model Loss', fontsize = 20)\n",
    "plt.ylabel('Loss', fontsize = 16)\n",
    "plt.xlabel('Epoch', fontsize = 16)\n",
    "plt.legend(['train', 'test'], loc='upper left', fontsize = 16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WbbgJMiVQ7pW"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "005_CNN_Model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
